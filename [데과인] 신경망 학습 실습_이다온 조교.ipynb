{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daonly/personal/blob/main/%5B%EB%8D%B0%EA%B3%BC%EC%9D%B8%5D%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%ED%95%99%EC%8A%B5%20%EC%8B%A4%EC%8A%B5_%EC%9D%B4%EB%8B%A4%EC%98%A8%20%EC%A1%B0%EA%B5%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터과학과인공지능 7분반 실습\n",
        "Written by Daon Lee (onlyforres@gmail.com) on 09/25/2024"
      ],
      "metadata": {
        "id": "j9vUfHWfPgIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[신경망 학습]**"
      ],
      "metadata": {
        "id": "YEzo9ifMiP45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 학습: 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것"
      ],
      "metadata": {
        "id": "fQMQtW2Jiale"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriA1H9uX_pU"
      },
      "source": [
        "# 손실함수"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 신경망 학습에서 사용하는 지표"
      ],
      "metadata": {
        "id": "0VSusvzkjcRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mlqUnDEmklk",
        "outputId": "fb5f1003-6960-463e-c2cf-40a9ba47f17f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# folder = '데과인'\n",
        "folder = ''"
      ],
      "metadata": {
        "id": "ID8NO_A3mmFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4encvvOcbW03",
        "outputId": "2349f380-0db9-4968-e4b5-8b2084c2d583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(60000, 10)\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "sys.path.append(f'/content/drive/MyDrive/{folder}')\n",
        "\n",
        "import numpy as np\n",
        "from mnist import load_mnist\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = \\\n",
        "    load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "print(x_train.shape) # (60000, 784)\n",
        "print(t_train.shape) # 원-핫 인코딩 된 정답 레이블 (60000, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJx3F4gqsZbF"
      },
      "source": [
        "# 신경망에서의 기울기"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 전제\n",
        "\n",
        "    신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다. 신경망 학습은 다음과 같이 4단계로 수행한다.\n",
        "\n",
        "- 1단계 - 미니배치\n",
        "\n",
        "    훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실함수 값을 줄이는 것이 목표이다.\n",
        "\n",
        "- 2단계 - 기울기 산출\n",
        "\n",
        "    미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
        "\n",
        "- 3단계 - 매개변수 갱신\n",
        "\n",
        "    가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
        "\n",
        "- 4단계 - 반복\n",
        "\n",
        "    1~3단계를 반복한다.\n",
        "\n",
        "- 데이터를 무작위로 선정하기 때문에 확률적 경사 하강법stochastic gradient descent,\n",
        "SGD라고 부른다.\n"
      ],
      "metadata": {
        "id": "fFTjr1e_w_GD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "++ 필요 함수들 정의"
      ],
      "metadata": {
        "id": "gon4PHICwOBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "zEfz6g1mx6Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T\n",
        "\n",
        "    x = x - np.max(x) # 오버플로 대책\n",
        "    return np.exp(x) / np.sum(np.exp(x))"
      ],
      "metadata": {
        "id": "yaq_6mwTv-Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "\n",
        "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size"
      ],
      "metadata": {
        "id": "Cy-pMGERwCRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_gradient(f, x):\n",
        "    h = 1e-4  # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x)  # f(x+h)\n",
        "\n",
        "        x[idx] = tmp_val - h\n",
        "        fxh2 = f(x)  # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "\n",
        "        x[idx] = tmp_val  # 값 복원\n",
        "        it.iternext()\n",
        "\n",
        "    return grad"
      ],
      "metadata": {
        "id": "h7ljRjbbwHOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######### 오차역전파법 ####### : sigmoid에 대한 오차역전파법. 다음 실습 때 쓰일 수도 ㅇㅇ.\n",
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)"
      ],
      "metadata": {
        "id": "ZK2B10cW2r4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk4nEUv6u8ZF"
      },
      "source": [
        "# 학습 알고리즘 구현해보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5b9ATZ9u_fa"
      },
      "outputs": [],
      "source": [
        "class TwoLayerNet:\n",
        "    \"\"\"\n",
        "    params : 신경망의 매개변수를 보관하는 딕셔너리 변수.\n",
        "    params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향.\n",
        "    params['W2']은 2번째 층의 가중치, params['b2']은 2번째 층의 편향.\n",
        "\n",
        "    grad : 기울기를 보관하는 딕셔너리 변수(numerical_gradient()의 반환값)\n",
        "    grads['W1']은 1번째 층의 가중치의 기울기, grads['b1']은 1번째 층의 편향의 기울기.\n",
        "    grads['W2']은 2번째 층의 가중치의 기울기, grads['b2']은 2번째 층의 편향의 기울기.\n",
        "    \"\"\"\n",
        "    # 초기화를 수행. input은 데이터의 크기, output size는 예측해야 할 class의 개수.\n",
        "    def __init__(self, input_size, hidden_size, output_size,\n",
        "                 weight_init_std=0.01):\n",
        "        # 매개변수 초기화 : 편향은 0으로, 가중치는 random으로 시작함. 이거 말고도 다양한 초기화 방법 존재.\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "            np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    # 예측(추론)을 수행한다. 이게 2층짜리 딥러닝 구조의 전부.\n",
        "    def predict(self, x):\n",
        "        W1, W2 = self.params['W1'], self.params['W2']\n",
        "        b1, b2 = self.params['b1'], self.params['b2']\n",
        "\n",
        "        a1 = np.dot(x, W1) + b1\n",
        "        z1 = sigmoid(a1)\n",
        "        a2 = np.dot(z1, W2) + b2\n",
        "        y = softmax(a2)\n",
        "\n",
        "        return y\n",
        "\n",
        "    # 손실 함수의 값을 구한다.\n",
        "    # x : 입력데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "\n",
        "        return cross_entropy_error(y, t)\n",
        "\n",
        "    # 정확도를 구한다. 결과가 나올 때 학습이 잘 이루어졌는지 볼 수 있는 지표.\n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        t = np.argmax(t, axis=1)\n",
        "\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "    # 가중치 매개변수의 기울기를 구한다.\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    # 이게 loss로 나온 결과를 수치미분하여 저장할 수 있는 코드.\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "        return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5rC-wTu1Sfn"
      },
      "source": [
        "# 미니배치 학습 구현\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EybR_7IQ3t5N",
        "outputId": "e959a5b2-a75a-44f8-c666-8a8e6e7f9847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(60000, 10)\n",
            "train acc, test acc | 0.11236666666666667, 0.1135\n",
            "train acc, test acc | 0.7923666666666667, 0.7972\n",
            "train acc, test acc | 0.87605, 0.8787\n",
            "train acc, test acc | 0.8968166666666667, 0.8983\n",
            "train acc, test acc | 0.9081166666666667, 0.9108\n",
            "train acc, test acc | 0.9136666666666666, 0.9151\n",
            "train acc, test acc | 0.9181, 0.9197\n",
            "train acc, test acc | 0.9224333333333333, 0.9252\n",
            "train acc, test acc | 0.9264666666666667, 0.9288\n",
            "train acc, test acc | 0.9306, 0.9312\n",
            "train acc, test acc | 0.9328666666666666, 0.9344\n",
            "train acc, test acc | 0.9349833333333334, 0.9353\n",
            "train acc, test acc | 0.9384, 0.9386\n",
            "train acc, test acc | 0.9400833333333334, 0.94\n",
            "train acc, test acc | 0.9428666666666666, 0.9409\n",
            "train acc, test acc | 0.9448166666666666, 0.9441\n",
            "train acc, test acc | 0.94635, 0.9444\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "sys.path.append(f'/content/drive/MyDrive/{folder}')\n",
        "\n",
        "import numpy as np\n",
        "from mnist import load_mnist\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = \\\n",
        "    load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "print(x_train.shape) # (60000, 784)  6만장이고, 784의 크기를 갖고 있음.\n",
        "print(t_train.shape) # 원-핫 인코딩 된 정답 레이블 (60000, 10)\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "# 하이퍼파라미터\n",
        "iters_num = 10000  # 이걸 설정해도 되고, epoch을 설정해줘도 됨.\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1  # 학습률 안 줄이고 계속 0.1 하나로 진행.\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)  # 두 개의 층을 갖는 딥러닝 구조.\n",
        "\n",
        "for i in range(iters_num):  # 이 iteration 수만큼 반복하게 됨.\n",
        "    batch_mask = np.random.choice(train_size, batch_size)  # 랜덤하게 뽑힌 100개를 사용. 근데 이거 하면 데이터가 반복될 수 있어서, 실제로 할 땐 batch를 안 겹치게 뽑는 다른 코드 사용함.\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    grad = network.numerical_gradient(x_batch, t_batch)\n",
        "\n",
        "    # 경사하강법을 통해 파라미터를 업데이트하는 단계\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:  # 몇번 돌 때마다 정확도 등의 정보를 뽑아낼 건지.\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}